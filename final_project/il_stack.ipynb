{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1335,"status":"ok","timestamp":1678405689000,"user":{"displayName":"Zhaofang Qian","userId":"05938139352712270314"},"user_tz":480},"id":"shSyf4FQPR22","outputId":"27d49348-c907-4861-85db-9802c08a4755"},"outputs":[],"source":["# Import required packages\n","import argparse\n","import os.path as osp\n","import pickle\n","import os\n","\n","import gym\n","import numpy as np\n","import h5py\n","import torch as th\n","import torch.nn as nn\n","from gym.wrappers import TimeLimit\n","from tqdm.notebook import tqdm\n","\n","import mani_skill2.envs\n","from mani_skill2.utils.wrappers import RecordEpisode\n","from torch.utils.data import Dataset, DataLoader\n"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1678405689001,"user":{"displayName":"Zhaofang Qian","userId":"05938139352712270314"},"user_tz":480},"id":"-vKVXwfrmz0O"},"outputs":[],"source":["env_id = \"StackCube-v1\""]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":15,"status":"ok","timestamp":1678405689006,"user":{"displayName":"Zhaofang Qian","userId":"05938139352712270314"},"user_tz":480},"id":"XMPZdMt6n2y8"},"outputs":[],"source":["# loads h5 data into memory for faster access\n","\n","def tensor_to_numpy(x):\n","    # moves all tensors to numpy. This is just for SB3 as SB3 does not optimize for observations stored on the GPU.\n","    if th.is_tensor(x):\n","        return x.cpu().numpy()\n","    return x\n","def convert_observation(observation):\n","    # flattens the original observation by flattening the state dictionaries\n","    # and combining the rgb and depth images\n","\n","    # image data is not scaled here and is kept as uint16 to save space\n","    image_obs = observation[\"image\"]\n","    rgb = image_obs[\"base_camera\"][\"rgb\"]\n","    depth = image_obs[\"base_camera\"][\"depth\"]\n","    rgb2 = image_obs[\"hand_camera\"][\"rgb\"]\n","    depth2 = image_obs[\"hand_camera\"][\"depth\"]\n","\n","    # we provide a simple tool to flatten dictionaries with state data\n","    from mani_skill2.utils.common import flatten_state_dict\n","    obs_ext = observation[\"extra\"][\"tcp_pose\"]\n","    state = np.hstack(\n","        [\n","            flatten_state_dict(observation[\"agent\"]),\n","            #obs_ext,\n","            flatten_state_dict(observation[\"extra\"]),\n","        ]\n","    )\n","    #print(state.shape)\n","    # combine the RGB and depth images\n","    rgbd = np.concatenate([rgb, depth, rgb2, depth2], axis=-1)\n","    obs = dict(rgbd=rgbd, state=state)\n","    return obs\n","def rescale_rgbd(rgbd):\n","    # rescales rgbd data and changes them to floats\n","    rgb1 = rgbd[..., 0:3] / 255.0\n","    rgb2 = rgbd[..., 4:7] / 255.0\n","    depth1 = rgbd[..., 3:4] / (2**10)\n","    depth2 = rgbd[..., 7:8] / (2**10) \n","    return np.concatenate([rgb1, depth1, rgb2, depth2], axis=-1)\n","class ManiSkill2Dataset(Dataset):\n","    def __init__(self, dataset_file: str, load_count=-1) -> None:\n","        self.dataset_file = dataset_file\n","        import pickle\n","        self.data = pickle.load(open(self.dataset_file, 'rb'))\n","        print('load in again')\n","        self.episodes = len(self.data.keys())\n","\n","        self.obs_state = []\n","        self.obs_rgbd = []\n","        self.actions = []\n","        self.total_frames = 0\n","        if load_count == -1:\n","            load_count = self.episodes\n","        for eps_id in tqdm(range(load_count)):\n","            # eps = self.episodes[eps_id]\n","            trajectory = self.data[f\"traj_{eps_id}\"]\n","\n","            # convert the original raw observation with our batch-aware function\n","            obs = convert_observation(trajectory[\"obs\"])\n","            # we use :-1 to ignore the last obs as terminal observations are included\n","            # and they don't have actions\n","            self.obs_rgbd.append(obs['rgbd'][:-1])\n","            self.obs_state.append(obs['state'][:-1])\n","            self.actions.append(trajectory[\"actions\"])\n","        #test = np.concatenate(self.obs_rgbd,axis=0)\n","        #print(f'shape of test{test.shape}')\n","        self.obs_rgbd = np.concatenate(self.obs_rgbd,axis=0)# np.vstack(self.obs_rgbd)\n","        #print(self.obs_rgbd.shape)\n","        self.obs_state =np.concatenate(self.obs_state,axis=0)# np.vstack(self.obs_state)\n","        self.actions = np.concatenate(self.actions,axis=0)#np.vstack(self.actions)\n","\n","    def __len__(self):\n","        return len(self.obs_rgbd)\n","\n","    def __getitem__(self, idx):\n","        action = th.from_numpy(self.actions[idx]).float()\n","        rgbd = self.obs_rgbd[idx]\n","        rgbd = rescale_rgbd(rgbd)\n","        # permute data so that channels are the first dimension as PyTorch expects this\n","        rgbd = th.from_numpy(rgbd).float().permute((2, 0, 1))\n","        state = th.from_numpy(self.obs_state[idx]).float()\n","        return dict(rgbd=rgbd, state=state), action\n","    "]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":84,"referenced_widgets":["98765ebd199049e88c7391131591fdc3","4b2fce93b9404a39bf65f3d0077baed8","d16e2aa5e4fd4525b18e9298c5ae36bc","71fbcd6061204d27ba98175393bffbdd","aecb7eea927a418fa76e269624dc9530","074d6d3ffaef4dacb780a07e7054150a","0f8521fdcb854408bf1a8c8f47543f2b","08bd020bd80044af986d6364842aacca","3859db33d4ff43feb016f135b434f7f7","d98dd2d314ac41a289160a736c34d317","90f13a5ec0db4963a8c63584a6d6fd37"]},"executionInfo":{"elapsed":32915,"status":"ok","timestamp":1678405721907,"user":{"displayName":"Zhaofang Qian","userId":"05938139352712270314"},"user_tz":480},"id":"t68wZsDYNzEA","outputId":"f1a43c77-b8d2-4598-eafa-b64abede555c"},"outputs":[{"name":"stdout","output_type":"stream","text":["load in again\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"783059e19f514972aab0082c4eea9edd","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/500 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["demopath = 'stackcube_500.pkl'\n","dataset = ManiSkill2Dataset(demopath)\n","dataloader = DataLoader(dataset, batch_size=100, num_workers=2, pin_memory=True, drop_last=True, shuffle=True)\n","obs, action = dataset[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":33,"status":"ok","timestamp":1678405721908,"user":{"displayName":"Zhaofang Qian","userId":"05938139352712270314"},"user_tz":480},"id":"8YH2JR18odbb","outputId":"b119b548-ae40-4b65-eaf8-a0087ac97203"},"outputs":[],"source":["\n","\n"]},{"cell_type":"markdown","metadata":{"id":"bGi5aYFFDx89"},"source":["These two using state in training phase"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":33,"status":"ok","timestamp":1678405722299,"user":{"displayName":"Zhaofang Qian","userId":"05938139352712270314"},"user_tz":480},"id":"WXsaGcMisXn3"},"outputs":[],"source":["class NatureCNN(nn.Module):\n","    def __init__(self, image_size=(128, 128), in_channels=8, state_size=32):\n","        super().__init__()\n","\n","        extractors = {}\n","\n","        self.out_features = 0\n","        feature_size = 256\n","\n","        # here we use a NatureCNN architecture to process images, but any architecture is permissble here\n","        cnn = nn.Sequential(\n","            nn.Conv2d(in_channels=in_channels, out_channels=32, kernel_size=8, stride=4, padding=0),\n","            nn.ReLU(),\n","            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2, padding=0),\n","            nn.ReLU(),\n","            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=0),\n","            nn.ReLU(),\n","            nn.Flatten()\n","        )\n","\n","        # to easily figure out the dimensions after flattening, we pass a test tensor\n","        test_tensor = th.zeros(\n","            [in_channels, image_size[0], image_size[1]]\n","        )\n","        with th.no_grad():\n","            n_flatten = cnn(test_tensor[None]).shape[1]\n","            fc = nn.Sequential(nn.Linear(n_flatten, feature_size), nn.ReLU())\n","        extractors[\"rgbd\"] = nn.Sequential(cnn, fc)\n","        self.out_features += feature_size   #image feature size would be 256\n","        \n","        # for state data we simply pass it through a single linear layer\n","        extractors[\"state\"] = nn.Linear(state_size, 64)\n","        self.out_features += 64\n","\n","        self.extractors = nn.ModuleDict(extractors)\n","\n","    def forward(self, observations) -> th.Tensor:\n","        encoded_tensor_list = []\n","        # self.extractors contain nn.Modules that do all the processing.\n","        for key, extractor in self.extractors.items():\n","            encoded_tensor_list.append(extractor(observations[key]))\n","        return th.cat(encoded_tensor_list, dim=1)"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":33,"status":"ok","timestamp":1678405722299,"user":{"displayName":"Zhaofang Qian","userId":"05938139352712270314"},"user_tz":480},"id":"I9cfmOkYoprL","outputId":"e0a8b62f-23fc-42b0-e530-d886e995dee2"},"outputs":[{"data":{"text/plain":["'\\n# create our policy\\nobs, action = dataset[0]\\nrgbd_shape = obs[\\'rgbd\\'].shape\\nprint(obs[\\'rgbd\\'].shape)\\nth.manual_seed(0)\\npolicy = Policy(image_size=rgbd_shape[1:], in_channels=rgbd_shape[0], state_size=obs[\\'state\\'].shape[0], \\n                act_dims=action.shape[0], hidden_units=[256, 256, 256])\\n# move model to gpu if possible\\ndevice = \"cuda\" if th.cuda.is_available() else \"cpu\"\\npolicy = policy.to(device)\\nprint(policy)\\n'"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["class Policy(nn.Module):\n","    def __init__(\n","        self,\n","        image_size=(128, 128),\n","        in_channels=8,\n","        state_size=42,\n","        hidden_units=[128, 128],\n","        act_dims=8,\n","        activation=nn.ReLU,\n","    ):\n","        super().__init__()\n","        self.feature_extractor = NatureCNN(image_size, in_channels, state_size)\n","        mlp_layers = []\n","        prev_units = self.feature_extractor.out_features\n","        for h in hidden_units:\n","            mlp_layers += [nn.Linear(prev_units, h), activation()]\n","            prev_units = h\n","        mlp_layers += [nn.Linear(prev_units, act_dims), nn.Tanh()]\n","        self.mlp = nn.Sequential(*mlp_layers)\n","\n","    def forward(self, observations) -> th.Tensor:\n","        features = self.feature_extractor(observations)\n","        return self.mlp(features)\n","'''\n","# create our policy\n","obs, action = dataset[0]\n","rgbd_shape = obs['rgbd'].shape\n","print(obs['rgbd'].shape)\n","th.manual_seed(0)\n","policy = Policy(image_size=rgbd_shape[1:], in_channels=rgbd_shape[0], state_size=obs['state'].shape[0], \n","                act_dims=action.shape[0], hidden_units=[256, 256, 256])\n","# move model to gpu if possible\n","device = \"cuda\" if th.cuda.is_available() else \"cpu\"\n","policy = policy.to(device)\n","print(policy)\n","'''"]},{"cell_type":"markdown","metadata":{"id":"VJ-ljIGaD256"},"source":["These new network is using without state info, pure rgbd data "]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":26,"status":"ok","timestamp":1678405722300,"user":{"displayName":"Zhaofang Qian","userId":"05938139352712270314"},"user_tz":480},"id":"7aK2cIYmD8jV"},"outputs":[],"source":["class PureCNN(nn.Module):\n","    def __init__(self, image_size=(128, 128), in_channels=8):\n","        super().__init__()\n","\n","        extractors = {}\n","\n","        self.out_features = 0\n","        feature_size = 256\n","\n","        # here we use a NatureCNN architecture to process images, but any architecture is permissble here\n","        cnn = nn.Sequential(\n","            nn.Conv2d(in_channels=in_channels, out_channels=32, kernel_size=8, stride=4, padding=0),\n","            nn.ReLU(),\n","            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2, padding=0),\n","            nn.ReLU(),\n","            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=0),\n","            nn.ReLU(),\n","            nn.Flatten()\n","        )\n","\n","        # to easily figure out the dimensions after flattening, we pass a test tensor\n","        test_tensor = th.zeros(\n","            [in_channels, image_size[0], image_size[1]]\n","        )\n","        with th.no_grad():\n","            n_flatten = cnn(test_tensor[None]).shape[1]\n","            fc = nn.Sequential(nn.Linear(n_flatten, feature_size), nn.ReLU())\n","        extractors[\"rgbd\"] = nn.Sequential(cnn, fc)\n","        self.out_features += feature_size   #image feature size would be 256\n","        \n","        # for state data we simply pass it through a single linear layer\n","        #extractors[\"state\"] = nn.Linear(state_size, 64)\n","        #self.out_features += 64\n","\n","        self.extractors = nn.ModuleDict(extractors)\n","\n","    def forward(self, observations) -> th.Tensor:\n","        encoded_tensor_list = []\n","        # self.extractors contain nn.Modules that do all the processing.\n","        for key, extractor in self.extractors.items():\n","            encoded_tensor_list.append(extractor(observations[key]))\n","        return th.cat(encoded_tensor_list, dim=1)"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26,"status":"ok","timestamp":1678405722300,"user":{"displayName":"Zhaofang Qian","userId":"05938139352712270314"},"user_tz":480},"id":"EucDiJSgEUeZ","outputId":"11d23404-8d08-47f6-ae66-854e08c41f1b"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([55])\n","VisPolicy(\n","  (feature_extractor): PureCNN(\n","    (extractors): ModuleDict(\n","      (rgbd): Sequential(\n","        (0): Sequential(\n","          (0): Conv2d(8, 32, kernel_size=(8, 8), stride=(4, 4))\n","          (1): ReLU()\n","          (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n","          (3): ReLU()\n","          (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n","          (5): ReLU()\n","          (6): Flatten(start_dim=1, end_dim=-1)\n","        )\n","        (1): Sequential(\n","          (0): Linear(in_features=9216, out_features=256, bias=True)\n","          (1): ReLU()\n","        )\n","      )\n","    )\n","  )\n","  (mlp): Sequential(\n","    (0): Linear(in_features=256, out_features=256, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=256, out_features=256, bias=True)\n","    (3): ReLU()\n","    (4): Linear(in_features=256, out_features=256, bias=True)\n","    (5): ReLU()\n","    (6): Linear(in_features=256, out_features=7, bias=True)\n","    (7): Tanh()\n","  )\n",")\n"]}],"source":["class VisPolicy(nn.Module):\n","    def __init__(\n","        self,\n","        image_size=(128, 128),\n","        in_channels=8,\n","        hidden_units=[128, 128],\n","        act_dims=8,\n","        activation=nn.ReLU,\n","    ):\n","        super().__init__()\n","        self.feature_extractor = PureCNN(image_size, in_channels)\n","        mlp_layers = []\n","        prev_units = self.feature_extractor.out_features\n","        for h in hidden_units:\n","            mlp_layers += [nn.Linear(prev_units, h), activation()]\n","            prev_units = h\n","        mlp_layers += [nn.Linear(prev_units, act_dims), nn.Tanh()]\n","        self.mlp = nn.Sequential(*mlp_layers)\n","\n","    def forward(self, observations) -> th.Tensor:\n","        features = self.feature_extractor(observations)\n","        return self.mlp(features)\n","\n","# create our policy\n","obs, action = dataset[0]\n","rgbd_shape = obs['rgbd'].shape\n","print(obs['state'].shape)\n","th.manual_seed(0)\n","policy = VisPolicy(image_size=rgbd_shape[1:], in_channels=rgbd_shape[0],\n","                act_dims=action.shape[0], hidden_units=[256, 256, 256])\n","# move model to gpu if possible\n","device = \"cuda\" if th.cuda.is_available() else \"cpu\"\n","policy = policy.to(device)\n","print(policy)"]},{"cell_type":"markdown","metadata":{"id":"f2X_ey7Luxcz"},"source":["### 2.4 Setting up Training, Dataloader, and Logging\n","\n","With a policy and dataset, we can now write some utility functions to perform a training step, load data in batches, and log results to tensorboard."]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":24,"status":"ok","timestamp":1678405722301,"user":{"displayName":"Zhaofang Qian","userId":"05938139352712270314"},"user_tz":480},"id":"P0r6hcx3uxc1"},"outputs":[],"source":["loss_fn = nn.MSELoss()\n","\n","# a short save function to save our model\n","def save_model(policy, path):\n","    save_data = dict(\n","        policy=policy.state_dict(),\n","    )\n","    base = os.path.join('./model',path)\n","    th.save(save_data, base)\n","\n","def train_step(policy, obs, actions, optim, loss_fn):\n","    optim.zero_grad()\n","    # move data to appropriate device first\n","    obs_device = dict()\n","    for k in obs:\n","        obs_device[k] = obs[k].to(device)\n","    actions = actions.to(device)\n","\n","    pred_actions = policy(obs_device)\n","    \n","    # compute loss and optimize\n","    loss = loss_fn(actions, pred_actions)\n","    loss.backward()\n","    optim.step()\n","    return loss.item()"]},{"cell_type":"markdown","metadata":{"id":"1_Ivtp54uxc2"},"source":["Below sets up the logging tools as well which can be viewed with `tensorboard --logdir logs`. You can also open up Tensorboard directly in this notebook"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":226,"status":"ok","timestamp":1678406225471,"user":{"displayName":"Zhaofang Qian","userId":"05938139352712270314"},"user_tz":480},"id":"HBP6bq5vuxc2"},"outputs":[],"source":["from torch.utils.tensorboard import SummaryWriter\n","\n","#writer = SummaryWriter(os.path.join('/content/drive/MyDrive/Colab_Notebooks/final/liftcube',f\"logs/rgbd_{env_id}\"))\n","writer = SummaryWriter(f\"./logs/rgbd_{env_id}_dagger\")"]},{"cell_type":"markdown","metadata":{"id":"9VXjk-z8uxc3"},"source":["### 2.5 Training\n","\n","We can now create a optimizer and training loop and begin training. The code below will optimize for `iterations = 8000` number of gradient steps at a learning rate of `1e-3`. These parameters are tuned for training on the LiftCube environment and will train a succesful policy that doesn't overfit too much to the dataset. Training time takes around 5-25 minutes depending on hardware.\n","\n","Note that this is a simple tutorial with a barebones training setup. It doesn't include using a validation dataset, computing success rate during training, regularization or normalization etc.\n","\n","See final thoughts in section 3 for some simple suggestions on how improve the IL approach."]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["8cdf21982f924baa9f563110dca142b5","a0100b66b6af4e159c3bd145c6e75935","3a8424d1460c4acea4dd3d07b2457da6","b05a4936c3f0457fadd8fe5973b8536c","212aafee14eb4cf583c639cc17dbf095","bfa0768e35a6411ca8f3d18f86dd7c0f","ab299286dd80438fb84f27724ce07821","6e081a2da7b3430880c8fdde9410775a","ac8c12db07334df7a8b1818d73cda447","e1ae1162874e428c9d647a2724279def","e90664e19e3e4adea6ddcc483ed1825b"]},"executionInfo":{"elapsed":2974270,"status":"ok","timestamp":1678409224981,"user":{"displayName":"Zhaofang Qian","userId":"05938139352712270314"},"user_tz":480},"id":"R1CSVd_Cuxc4","outputId":"55d26b1f-3f79-44c8-af06-000dc869be58"},"outputs":[{"data":{"text/plain":["'\\niterations = 5000\\noptim = th.optim.Adam(policy.parameters(), lr=1e-3)\\nbest_epoch_loss = np.inf\\npbar = tqdm(dataloader, total=iterations)\\nepoch = 0\\nsteps = 0\\nwhile steps < iterations:\\n    epoch_loss = 0\\n    for batch in dataloader:\\n        steps += 1\\n        obs, actions = batch\\n        loss_val = train_step(policy, obs, actions, optim, loss_fn)\\n\\n        # track the loss and print it\\n        writer.add_scalar(\"train/mse_loss\", loss_val, steps)\\n        epoch_loss += loss_val\\n        pbar.set_postfix(dict(loss=loss_val))\\n        pbar.update(1)\\n\\n        # periodically save the policy\\n        if steps % 1000 == 0: save_model(policy, f\"ckpt_{steps}_stack.pt\")\\n        if steps >= iterations: break\\n    \\n    epoch_loss = epoch_loss / len(dataloader)\\n\\n    # save a new model if the average MSE loss in an epoch has improved\\n    if epoch_loss < best_epoch_loss:\\n        best_epoch_loss = epoch_loss\\n        save_model(policy, \"ckpt_best1_stack.pt\")\\n    \\n    writer.add_scalar(\"train/mse_loss_epoch\", epoch_loss, epoch)\\n    epoch += 1\\nsave_model(policy, \"ckpt_latest_stack.pt\")\\n'"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["'''\n","iterations = 5000\n","optim = th.optim.Adam(policy.parameters(), lr=1e-3)\n","best_epoch_loss = np.inf\n","pbar = tqdm(dataloader, total=iterations)\n","epoch = 0\n","steps = 0\n","while steps < iterations:\n","    epoch_loss = 0\n","    for batch in dataloader:\n","        steps += 1\n","        obs, actions = batch\n","        loss_val = train_step(policy, obs, actions, optim, loss_fn)\n","\n","        # track the loss and print it\n","        writer.add_scalar(\"train/mse_loss\", loss_val, steps)\n","        epoch_loss += loss_val\n","        pbar.set_postfix(dict(loss=loss_val))\n","        pbar.update(1)\n","\n","        # periodically save the policy\n","        if steps % 1000 == 0: save_model(policy, f\"ckpt_{steps}_stack.pt\")\n","        if steps >= iterations: break\n","    \n","    epoch_loss = epoch_loss / len(dataloader)\n","\n","    # save a new model if the average MSE loss in an epoch has improved\n","    if epoch_loss < best_epoch_loss:\n","        best_epoch_loss = epoch_loss\n","        save_model(policy, \"ckpt_best1_stack.pt\")\n","    \n","    writer.add_scalar(\"train/mse_loss_epoch\", epoch_loss, epoch)\n","    epoch += 1\n","save_model(policy, \"ckpt_latest_stack.pt\")\n","'''"]},{"cell_type":"markdown","metadata":{"id":"-Hvgtqgwuxc4"},"source":["### 2.6 Evaluation\n","\n","With a trained policy on our hands, we can now create an evaluation environment to compute the success rate and watch the videos. The default settings should train a policy that achieves around 30% success rate"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":531,"status":"ok","timestamp":1678409232186,"user":{"displayName":"Zhaofang Qian","userId":"05938139352712270314"},"user_tz":480},"id":"lToTPPw9uxc4","outputId":"54f5274c-52a6-4330-cbd4-4f98472bc391"},"outputs":[{"data":{"text/plain":["'\\npath = os.path.join(\\'/content/drive/MyDrive/Colab_Notebooks/final/\\',\"ckpt_best1_stack.pt\")\\npolicy.load_state_dict(th.load(path)[\"policy\"])\\n'"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["# optionally load a checkpoint\n","'''\n","path = os.path.join('/content/drive/MyDrive/Colab_Notebooks/final/',\"ckpt_best1_stack.pt\")\n","policy.load_state_dict(th.load(path)[\"policy\"])\n","'''"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":235},"executionInfo":{"elapsed":1168,"status":"error","timestamp":1678409243254,"user":{"displayName":"Zhaofang Qian","userId":"05938139352712270314"},"user_tz":480},"id":"pcBhlMD3uxc5","outputId":"4a64d48d-0273-4cc1-aa14-1f06786e7d59"},"outputs":[{"data":{"text/plain":["'\\nenv_id=\"StackCube-v1\"\\nobs_mode = \"rgbd\"\\ncontrol_mode = \"pd_ee_delta_pose\"\\nenv = gym.make(env_id, obs_mode=obs_mode, control_mode=control_mode)\\n# RecordEpisode wrapper auto records a new video once an episode is completed\\nenv = RecordEpisode(env, output_dir=f\"logs/rgbd_{env_id}/videos\")\\nobs = env.reset(seed=42)\\n\\nsuccesses = []\\nnum_episodes = 100\\ni = 0\\npbar = tqdm(total=num_episodes)\\nwhile i < num_episodes:\\n    # convert observation to our desired shape and move to appropriate device\\n    obs = convert_observation(obs)\\n    obs_device = dict()\\n    obs[\\'rgbd\\'] = rescale_rgbd(obs[\\'rgbd\\'])\\n    # unsqueeze adds an extra batch dimension and we permute rgbd since PyTorch expects the channel dimension to be first\\n    obs_device[\\'rgbd\\'] = th.from_numpy(obs[\\'rgbd\\']).float().permute(2,0,1).unsqueeze(0).to(device)\\n    obs_device[\\'state\\'] = th.from_numpy(obs[\\'state\\']).float().unsqueeze(0).to(device)\\n    #print(obs[\\'state\\'])\\n    with th.no_grad():\\n        action = policy(obs_device).cpu().numpy()[0]\\n    obs, reward, done, info = env.step(action)\\n\\n    if done:\\n        successes.append(info[\\'success\\'])\\n        obs = env.reset()\\n        i += 1\\n        pbar.update(1)\\nprint(\"Success Rate:\", np.mean(successes))\\nprint(successes)\\nwriter.add_scalar(\"Success Rate\",  np.mean(successes), num_episodes)\\n'"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["from mani_skill2.utils.wrappers import RecordEpisode\n","'''\n","env_id=\"StackCube-v1\"\n","obs_mode = \"rgbd\"\n","control_mode = \"pd_ee_delta_pose\"\n","env = gym.make(env_id, obs_mode=obs_mode, control_mode=control_mode)\n","# RecordEpisode wrapper auto records a new video once an episode is completed\n","env = RecordEpisode(env, output_dir=f\"logs/rgbd_{env_id}/videos\")\n","obs = env.reset(seed=42)\n","\n","successes = []\n","num_episodes = 100\n","i = 0\n","pbar = tqdm(total=num_episodes)\n","while i < num_episodes:\n","    # convert observation to our desired shape and move to appropriate device\n","    obs = convert_observation(obs)\n","    obs_device = dict()\n","    obs['rgbd'] = rescale_rgbd(obs['rgbd'])\n","    # unsqueeze adds an extra batch dimension and we permute rgbd since PyTorch expects the channel dimension to be first\n","    obs_device['rgbd'] = th.from_numpy(obs['rgbd']).float().permute(2,0,1).unsqueeze(0).to(device)\n","    obs_device['state'] = th.from_numpy(obs['state']).float().unsqueeze(0).to(device)\n","    #print(obs['state'])\n","    with th.no_grad():\n","        action = policy(obs_device).cpu().numpy()[0]\n","    obs, reward, done, info = env.step(action)\n","\n","    if done:\n","        successes.append(info['success'])\n","        obs = env.reset()\n","        i += 1\n","        pbar.update(1)\n","print(\"Success Rate:\", np.mean(successes))\n","print(successes)\n","writer.add_scalar(\"Success Rate\",  np.mean(successes), num_episodes)\n","'''"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["class DAggerdataset(Dataset):\n","    def __init__(self,data):\n","       \n","        self.obs_state = []\n","        self.obs_rgbd = []\n","        self.actions = []\n","        \n","        for _,v in data.items():    #each k is 'traj_1\" ,v would be a dict\n","            \n","            \n","            obs = convert_observation(v[\"obs\"])\n","            self.obs_rgbd.append(obs['rgbd'][:-1])\n","            self.obs_state.append(obs['state'][:-1])\n","            self.actions.append(v[\"actions\"])\n","        \n","        self.obs_rgbd = np.concatenate(self.obs_rgbd,axis=0)# np.vstack(self.obs_rgbd)\n","        \n","        self.obs_state =np.concatenate(self.obs_state,axis=0)# np.vstack(self.obs_state)\n","        self.actions = np.concatenate(self.actions,axis=0)#np.vstack(self.actions)\n","\n","\n","\n","    def __len__(self):\n","        return len(self.obs_rgbd)\n","    def __getitem__(self,idx):\n","\n","        action = th.from_numpy(self.actions[idx]).float()\n","        rgbd = self.obs_rgbd[idx]\n","        rgbd = rescale_rgbd(rgbd)\n","        # permute data so that channels are the first dimension as PyTorch expects this\n","        rgbd = th.from_numpy(rgbd).float().permute((2, 0, 1))\n","        state = th.from_numpy(self.obs_state[idx]).float()\n","        return dict(rgbd=rgbd, state=state), action\n"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["def trainBC(dataloader,iterations,policy,loss_fn,writer):\n","    \n","    optim = th.optim.Adam(policy.parameters(), lr=1e-3)\n","    best_epoch_loss = np.inf\n","    pbar = tqdm(dataloader, total=iterations)\n","    epoch = 0\n","    steps = 0\n","    while steps < iterations:\n","        epoch_loss = 0\n","        for batch in dataloader:\n","            \n","            steps += 1\n","            obs, actions = batch\n","            loss_val = train_step(policy, obs, actions, optim, loss_fn)\n","\n","            # track the loss and print it\n","            writer.add_scalar(\"train/mse_loss\", loss_val, steps)\n","            epoch_loss += loss_val\n","            pbar.set_postfix(dict(loss=loss_val))\n","            pbar.update(1)\n","\n","            # periodically save the policy\n","            if steps % 1000 == 0: save_model(policy, f\"stack/ckpt_{steps}.pt\")\n","            if steps >= iterations: break\n","        \n","        epoch_loss = epoch_loss / len(dataloader)\n","\n","        # save a new model if the average MSE loss in an epoch has improved\n","        if epoch_loss < best_epoch_loss:\n","            best_epoch_loss = epoch_loss\n","            save_model(policy, \"stack/ckpt_best.pt\")\n","        \n","        writer.add_scalar(\"train/mse_loss_epoch\", epoch_loss, epoch)\n","        epoch += 1\n","    save_model(policy, \"stack/ckpt_latest.pt\")\n","    return policy"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"76b504875a9a46269451f10d56cef6e4","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/100 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Rollout 0, Number of total traj: 108, Number of total traj from dictdata 500\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7fb979e5ec8e49f7aa230fcae168ef13","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/5000 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Finish training student..\n"]},{"ename":"AttributeError","evalue":"'CollectTrajectories' object has no attribute 'tcp_pose'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[16], line 80\u001b[0m\n\u001b[1;32m     77\u001b[0m testdata \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mload(\u001b[39m'\u001b[39m\u001b[39m./stackcube_500.pkl\u001b[39m\u001b[39m'\u001b[39m,allow_pickle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     78\u001b[0m \u001b[39m#print(testdata['traj_0']['obs'].keys())\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[39m#dataload = DAggerdataset(testdata)\u001b[39;00m\n\u001b[0;32m---> 80\u001b[0m dagger(testdata,expert,env,policy,writer)\n","Cell \u001b[0;32mIn[16], line 55\u001b[0m, in \u001b[0;36mdagger\u001b[0;34m(dictdata, expert, env, policy, writer)\u001b[0m\n\u001b[1;32m     52\u001b[0m expert_action,_ \u001b[39m=\u001b[39m expert\u001b[39m.\u001b[39mpredict(obs_device[\u001b[39m'\u001b[39m\u001b[39mstate\u001b[39m\u001b[39m'\u001b[39m])     \u001b[39m#expert's action under this observation, state-based obs\u001b[39;00m\n\u001b[1;32m     53\u001b[0m pred_action \u001b[39m=\u001b[39m student(obs_device)\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()[\u001b[39m0\u001b[39m]       \u001b[39m#image-based obs \u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m traj_collect\u001b[39m.\u001b[39;49mupdate_extras(obs,expert_action,done)\n\u001b[1;32m     58\u001b[0m obs, r, done, _ \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(pred_action)\n\u001b[1;32m     60\u001b[0m totalr \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m r\n","File \u001b[0;32m~/Study/CSE291A_23Win/final_project/trajectory.py:84\u001b[0m, in \u001b[0;36mCollectTrajectories.update_extras\u001b[0;34m(self, observation, action, done)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mupdate_extras\u001b[39m(\u001b[39mself\u001b[39m,observation,action,done):\n\u001b[0;32m---> 84\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mupdate(observation,action,done)\n\u001b[1;32m     85\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mLiftCube-v1\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m     86\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtcp_pose\u001b[39m.\u001b[39mappend( observation[\u001b[39m'\u001b[39m\u001b[39mextra\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mtcp_pose\u001b[39m\u001b[39m'\u001b[39m])\n","File \u001b[0;32m~/Study/CSE291A_23Win/final_project/trajectory.py:33\u001b[0m, in \u001b[0;36mGetObs.update\u001b[0;34m(self, observation, action, done)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mqvel\u001b[39m.\u001b[39mappend(observation[\u001b[39m'\u001b[39m\u001b[39magent\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mqvel\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     32\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_pose\u001b[39m.\u001b[39mappend(  observation[\u001b[39m'\u001b[39m\u001b[39magent\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mbase_pose\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> 33\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtcp_pose\u001b[39m.\u001b[39mappend( observation[\u001b[39m'\u001b[39m\u001b[39mextra\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mtcp_pose\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     34\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj_pose\u001b[39m.\u001b[39mappend( observation[\u001b[39m'\u001b[39m\u001b[39mextra\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mobj_pose\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     35\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtcp_to_obj_pos\u001b[39m.\u001b[39mappend( observation[\u001b[39m'\u001b[39m\u001b[39mextra\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mtcp_to_obj_pos\u001b[39m\u001b[39m'\u001b[39m])\n","\u001b[0;31mAttributeError\u001b[0m: 'CollectTrajectories' object has no attribute 'tcp_pose'"]}],"source":["from stable_baselines3 import SAC\n","from tqdm import trange\n","from trajectory import CollectTrajectories\n","\n","env_id=\"StackCube-v1\"\n","obs_mode = \"rgbd\"\n","control_mode = \"pd_ee_delta_pose\"\n","env = gym.make(env_id, obs_mode=obs_mode, control_mode=control_mode)\n","obs = env.reset()\n","\n","#print(obs['extra'].keys())\n","expert = SAC.load('./logs/stackcube_latest_model.zip',device=device)\n","\n","def dagger(dictdata,expert,env,policy,writer):       \n","    #expert,student would be two policy\n","    \n","    num_rollout = 20\n","    loss_fn =  nn.MSELoss()\n","    for i in tqdm(range(100)):   #100 new rounds/traj\n","        traj_dataset = DAggerdataset(dictdata)\n","        new_traj = DataLoader(traj_dataset, batch_size=100, num_workers=16, pin_memory=True, drop_last=True, shuffle=True)\n","        \n","        new_obs = {}\n","        \n","        reward_per_round = []\n","        \n","        max_steps = 5000\n","        print(f'Rollout {i}, Number of total traj: {len(new_traj)}, Number of total traj from dictdata {len(dictdata.keys())}')\n","        student = trainBC(dataloader =new_traj,policy=policy,loss_fn = loss_fn,writer=writer,iterations=max_steps)\n","        print('Finish training student..')\n","        for i in range(num_rollout):\n","            \n","            obs = env.reset()\n","            done = False\n","            totalr = 0.\n","            steps = 0\n","            obstemp = convert_observation(obs)\n","            preact = th.from_numpy(obstemp['state']).float().unsqueeze(0)\n","            expert_action,_ = expert.predict(preact)\n","            traj_collect = CollectTrajectories(obs, expert_action,env)\n","            \n","            while not done:\n","                #move data to GPU\n","                obstemp = convert_observation(obs)\n","                obs_device = dict()\n","                obs['rgbd'] = rescale_rgbd(obstemp['rgbd'])\n","                # unsqueeze adds an extra batch dimension and we permute rgbd since PyTorch expects the channel dimension to be first\n","                obs_device['rgbd'] = th.from_numpy(obstemp['rgbd']).float().permute(2,0,1).unsqueeze(0).to(device)\n","                obs_device['state'] = th.from_numpy(obstemp['state']).float().unsqueeze(0)\n","                \n","\n","                expert_action,_ = expert.predict(obs_device['state'])     #expert's action under this observation, state-based obs\n","                pred_action = student(obs_device).detach().cpu().numpy()[0]       #image-based obs \n","                \n","                traj_collect.update_extras(obs,expert_action,done)\n","\n","\n","                obs, r, done, _ = env.step(pred_action)\n","\n","                totalr += r\n","                steps += 1\n","               \n","                if steps > max_steps:\n","                    break\n","            reward_per_round.append(totalr)\n","            \n","            \n","        \n","\n","        print('returns', reward_per_round)\n","        print('mean return', np.mean(reward_per_round))\n","        print('std of return', np.std(reward_per_round))\n","       \n","        dictdata[f'traj_{i+100}'] = traj_collect.trajectory()\n","        \n","    return new_traj\n","testdata = np.load('./stackcube_500.pkl',allow_pickle=True)\n","#print(testdata['traj_0']['obs'].keys())\n","#dataload = DAggerdataset(testdata)\n","dagger(testdata,expert,env,policy,writer)\n","        \n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1ZECzF66ByMxDm4CinzCnh5pJVcbwkCQt","timestamp":1678398504327},{"file_id":"https://github.com/haosulab/ManiSkill2/blob/main/examples/tutorials/3_imitation_learning.ipynb","timestamp":1678329509714}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"vscode":{"interpreter":{"hash":"916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"}},"widgets":{"application/vnd.jupyter.widget-state+json":{"074d6d3ffaef4dacb780a07e7054150a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"08bd020bd80044af986d6364842aacca":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0f8521fdcb854408bf1a8c8f47543f2b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"212aafee14eb4cf583c639cc17dbf095":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3859db33d4ff43feb016f135b434f7f7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3a8424d1460c4acea4dd3d07b2457da6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_6e081a2da7b3430880c8fdde9410775a","max":5000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ac8c12db07334df7a8b1818d73cda447","value":5000}},"4b2fce93b9404a39bf65f3d0077baed8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_074d6d3ffaef4dacb780a07e7054150a","placeholder":"​","style":"IPY_MODEL_0f8521fdcb854408bf1a8c8f47543f2b","value":"100%"}},"6e081a2da7b3430880c8fdde9410775a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"71fbcd6061204d27ba98175393bffbdd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d98dd2d314ac41a289160a736c34d317","placeholder":"​","style":"IPY_MODEL_90f13a5ec0db4963a8c63584a6d6fd37","value":" 200/200 [00:15&lt;00:00, 18.81it/s]"}},"8cdf21982f924baa9f563110dca142b5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a0100b66b6af4e159c3bd145c6e75935","IPY_MODEL_3a8424d1460c4acea4dd3d07b2457da6","IPY_MODEL_b05a4936c3f0457fadd8fe5973b8536c"],"layout":"IPY_MODEL_212aafee14eb4cf583c639cc17dbf095"}},"90f13a5ec0db4963a8c63584a6d6fd37":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"98765ebd199049e88c7391131591fdc3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4b2fce93b9404a39bf65f3d0077baed8","IPY_MODEL_d16e2aa5e4fd4525b18e9298c5ae36bc","IPY_MODEL_71fbcd6061204d27ba98175393bffbdd"],"layout":"IPY_MODEL_aecb7eea927a418fa76e269624dc9530"}},"a0100b66b6af4e159c3bd145c6e75935":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bfa0768e35a6411ca8f3d18f86dd7c0f","placeholder":"​","style":"IPY_MODEL_ab299286dd80438fb84f27724ce07821","value":"100%"}},"ab299286dd80438fb84f27724ce07821":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ac8c12db07334df7a8b1818d73cda447":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"aecb7eea927a418fa76e269624dc9530":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b05a4936c3f0457fadd8fe5973b8536c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e1ae1162874e428c9d647a2724279def","placeholder":"​","style":"IPY_MODEL_e90664e19e3e4adea6ddcc483ed1825b","value":" 5000/5000 [49:33&lt;00:00,  1.93it/s, loss=0.00765]"}},"bfa0768e35a6411ca8f3d18f86dd7c0f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d16e2aa5e4fd4525b18e9298c5ae36bc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_08bd020bd80044af986d6364842aacca","max":200,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3859db33d4ff43feb016f135b434f7f7","value":200}},"d98dd2d314ac41a289160a736c34d317":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e1ae1162874e428c9d647a2724279def":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e90664e19e3e4adea6ddcc483ed1825b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}
